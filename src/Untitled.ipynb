{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3220eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import Random\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import traceback\n",
    "from Annotation_forced import do_annotation\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "from itertools import product\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "ITEMS_PER_CONDITION = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9cb341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, prompts):\n",
    "        self.prompts = prompts\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]\n",
    "        \n",
    "models = [\n",
    "    # model name, batch_size, device, device Mapping\n",
    "    (\"stefan-it/german-gpt2-larger\", 2, 0, None),\n",
    "]\n",
    "\n",
    "with open(\"../items/names.json\", encoding=\"utf-8\") as nfile:\n",
    "    namedict = json.load(nfile)\n",
    "male_names = [name for name in namedict[\"male\"]]\n",
    "female_names = [name for name in namedict[\"female\"]]\n",
    "with open(\"../items/verbs_forced_reference.json\", encoding=\"utf-8\") as nfile:\n",
    "    verbdict = json.load(nfile)\n",
    "es_verbs = verbdict[\"es\"]\n",
    "se_verbs = verbdict[\"se\"]\n",
    "\n",
    "male_pairing = list(product(male_names, female_names, [False]))\n",
    "female_pairing = list(product(female_names, male_names, [True]))\n",
    "Random(42).shuffle(male_pairing)\n",
    "Random(84).shuffle(female_pairing)\n",
    "\n",
    "conditions = [\n",
    "    (2,  es_verbs, female_pairing, \"NP1\"),\n",
    "    (3,  es_verbs,   male_pairing, \"NP1\"),\n",
    "    # (6,  se_verbs, female_pairing, \"NP1\"),\n",
    "    # (7,  se_verbs,   male_pairing, \"NP1\"),\n",
    "    # (10, es_verbs, female_pairing, \"NP2\"),\n",
    "    # (11, es_verbs,   male_pairing, \"NP2\"),\n",
    "    (14, se_verbs, female_pairing, \"NP2\"),\n",
    "    (15, se_verbs,   male_pairing, \"NP2\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7120ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items_per_condition = []\n",
    "  \n",
    "for condition, verbs, pairing, forced_reference in conditions:\n",
    "    rows = []\n",
    "    for verbdict in verbs:\n",
    "        verb, filler, verbclass = verbdict[\"verb\"], verbdict[\"filler\"], verbdict[\"verbclass\"]\n",
    "        for np1, np2, female in pairing:\n",
    "            prompt = f\"{np1} {verb} {np2}{filler}, weil\"\n",
    "            nrow = {\"condition\": condition, \"type\": \"Experiment\", \"prompt\": prompt, \"NP1\": np1, \"NP2\": np2, \n",
    "                    \"NP1gender\": \"f\" if female else \"m\", \"verb\": verb, \"verbclass\": verbclass, \"forced\": forced_reference}\n",
    "            rows.append(nrow)\n",
    "    Random(168).shuffle(rows)\n",
    "    items_per_condition.append(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c95faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_constraint_function(tokenizer, female):\n",
    "    weil = tokenizer.encode(\", weil\")[-2:]\n",
    "    names = female_names if female else male_names\n",
    "    pronouns = [\" sie\", \" diese\", \" jense\"] if female else [\" er\", \" dieser\", \" jener\"] \n",
    "    tokens = list(map(model.tokenizer.encode, list(map(lambda name: \" \" + name, names)) + pronouns))\n",
    "    twos = [items for items in tokens if len(items) > 1]\n",
    "    twos_one = [item[0] for item in twos]\n",
    "    twos_two = [item[1] for item in twos]\n",
    "    all_tokens = list(range(tokenizer.vocab_size))\n",
    "    tokens = [item[0] for item in tokens]\n",
    "    def constrainer(batch_id, input_tokens):\n",
    "        if (input_tokens[-2] == weil[-2]) and (input_tokens[-1] == weil[-1]):\n",
    "            return tokens\n",
    "        elif (input_tokens[-3] == weil[-2]) and (input_tokens[-2] == weil[-1]) and (input_tokens[-1] in twos_one):\n",
    "            return [twos_two[twos_one.index(input_tokens[-1])]]\n",
    "        else:\n",
    "            return all_tokens\n",
    "    return constrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02ffcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loading: stefan-it/german-gpt2-larger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Condition 3:   0%|                                                                               | 0/8 [01:15<?, ?it/s]\u001b[A\n",
      "\n",
      "Condition 2:   0%|                                                                               | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Condition 2:  67%|███████████████████████████████████████████████▎                       | 2/3 [00:25<00:12, 12.60s/it]\u001b[A\n",
      "Condition 2: : 4it [00:48, 12.12s/it]                                                                                  \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 sentences for condition 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Condition 3: : 4it [00:47, 11.80s/it]                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 sentences for condition 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Condition 14: : 4it [00:46, 11.70s/it]                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 sentences for condition 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Condition 15: : 4it [00:44, 11.11s/it]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 sentences for condition 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, batch_size, device, device_map in models:\n",
    "    print(f\"now loading: {model_name}\")\n",
    "    model = pipeline(\"text-generation\", model = model_name)\n",
    "    model.tokenizer.pad_token_id = model.model.config.eos_token_id\n",
    "    model.tokenizer.padding_side = \"left\"    \n",
    "    data = []\n",
    "    for condition in items_per_condition:\n",
    "        items = deepcopy(condition)\n",
    "        constraint_function = make_constraint_function(model.tokenizer, False)\n",
    "        if ((condition[0][\"forced\"] == \"NP1\") and (condition[0][\"NP1gender\"] == \"f\")) or ((condition[0][\"forced\"] == \"NP2\") and (condition[0][\"NP1gender\"] == \"m\")):\n",
    "            constraint_function = make_constraint_function(model.tokenizer, True)\n",
    "        bar = tqdm(total = ITEMS_PER_CONDITION)\n",
    "        bar.set_description(f\"Condition {items[0]['condition']}\")\n",
    "        result = []\n",
    "        counter = 0\n",
    "        while bar.n < ITEMS_PER_CONDITION:\n",
    "            counter += 1\n",
    "            if len(items) >= batch_size:\n",
    "                rows = pd.DataFrame(items[:batch_size])\n",
    "                items = items[batch_size:]\n",
    "                prompts = rows[\"prompt\"].to_list()\n",
    "                continuations = model(prompts, batch_size=batch_size, remove_invalid_values=True, early_stopping = True, prefix_allowed_tokens_fn=constraint_function, do_sample = False, diversity_penalty = .6, num_beam_groups = 2, num_beams = 2, max_new_tokens = 20)\n",
    "                continuations = [cont[0][\"generated_text\"] for cont in continuations]\n",
    "                continuations = list(map(lambda zipped: zipped[1][len(zipped[0])+1:], zip(prompts, continuations)))\n",
    "                rows[\"cont\"] = continuations\n",
    "                res = do_annotation(rows, True)\n",
    "                res = res[res[\"Koreferenz\"] == res[\"forced\"]]\n",
    "                result += res.values.tolist()\n",
    "                bar.update(len(res))\n",
    "            else:\n",
    "                print(f\"Run out of data in condition {condition[0]['condition']}\")\n",
    "                break\n",
    "        del bar\n",
    "        print(f\"Generated {counter} sentences for condition {items[0]['condition']}\")\n",
    "        data += result\n",
    "\n",
    "    exp3 = pd.DataFrame(data, columns = [\"condition\", \"type\", \"prompt\", \"NP1\", \"NP2\", \"NP1gender\", \"verb\", \"verbclass\", \"forced\", \"cont\", \"Koreferenz\", \"Anaphorische Form\"])\n",
    "    exp3.to_csv(f\"../data/forced_coreference--{model_name.replace('/', '--')}.csv\", sep=\";\", index=False)\n",
    "    \n",
    "    del model\n",
    "    del exp3\n",
    "    del rows\n",
    "    del items\n",
    "    \n",
    "    # FIX ROW BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb30684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Annotation import do_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7256f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/coreference--ai-forever--mGPT.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645267d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"np1\":\"NP1\", \"np2\":\"NP2\", \"cat\":\"verbclass\", \"continuation\":\"cont\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e50409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"type\"] = [\"Experiment\"] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9abb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cont\"] = df.apply(lambda row: row[\"cont\"][len(row[\"prompt\"]) + 1:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed08aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NP1gender\"] = df[\"female\"].apply(lambda b: \"f\" if b else \"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e705ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/coreference--ai-forever--mGPT_annotated.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"../data/coreference--ai-forever--mGPT.csv\"[:-4] + \"_annotated.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "131249f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_annotation(df[:100], True).to_csv(\"hi.csv\", index=False, sep=\";\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
